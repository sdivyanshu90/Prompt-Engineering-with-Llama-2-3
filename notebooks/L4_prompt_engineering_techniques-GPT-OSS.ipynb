{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade cerebras_cloud_sdk","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-20T14:30:11.263604Z","iopub.execute_input":"2025-09-20T14:30:11.263835Z","iopub.status.idle":"2025-09-20T14:30:17.588438Z","shell.execute_reply.started":"2025-09-20T14:30:11.263817Z","shell.execute_reply":"2025-09-20T14:30:17.587521Z"}},"outputs":[{"name":"stdout","text":"Collecting cerebras_cloud_sdk\n  Downloading cerebras_cloud_sdk-1.50.1-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from cerebras_cloud_sdk) (4.9.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from cerebras_cloud_sdk) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from cerebras_cloud_sdk) (0.28.1)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from cerebras_cloud_sdk) (2.11.7)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from cerebras_cloud_sdk) (1.3.1)\nRequirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from cerebras_cloud_sdk) (4.14.0)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->cerebras_cloud_sdk) (3.10)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->cerebras_cloud_sdk) (2025.6.15)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->cerebras_cloud_sdk) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->cerebras_cloud_sdk) (0.16.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->cerebras_cloud_sdk) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->cerebras_cloud_sdk) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->cerebras_cloud_sdk) (0.4.1)\nDownloading cerebras_cloud_sdk-1.50.1-py3-none-any.whl (91 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.8/91.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: cerebras_cloud_sdk\nSuccessfully installed cerebras_cloud_sdk-1.50.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\ncerebras_api_key = user_secrets.get_secret(\"CEREBRAS_API\")\ngemini_api_key = user_secrets.get_secret(\"GOOGLE_API_KEY\")\nhf_api_key = user_secrets.get_secret(\"HF_API_KEY\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T14:30:17.590641Z","iopub.execute_input":"2025-09-20T14:30:17.590893Z","iopub.status.idle":"2025-09-20T14:30:17.865219Z","shell.execute_reply.started":"2025-09-20T14:30:17.590869Z","shell.execute_reply":"2025-09-20T14:30:17.864459Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os\nimport time\nfrom cerebras.cloud.sdk import Cerebras\n\nclient = Cerebras(api_key=cerebras_api_key)\n\n# Function to call GPT-OSS for text completion\ndef gpt_oss(prompt, \n            model=\"gpt-oss-120b\", \n            temperature=0.0, \n            max_tokens=1024,\n            verbose=False,\n            max_tries=3):\n    \n    # Show details if verbose mode is on\n    if verbose:\n        print(f\"Prompt:\\n{prompt}\\n\")\n        print(f\"Model: {model}\")\n        print(f\"Temperature: {temperature}\")\n        print(f\"Max Tokens: {max_tokens}\")\n\n    # Structure the message for the API\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": prompt,\n        }\n    ]\n    \n    # Allow multiple attempts to call the API in case of downtime or issues\n    for num_tries in range(max_tries):\n        try:\n            # Make the API call to Cerebras GPT-OSS model\n            chat_completion = client.chat.completions.create(\n                messages=messages,\n                model=model,\n                temperature=temperature,\n                max_tokens=max_tokens\n            )\n            \n            # Print the full response if verbose\n            if verbose:\n                print(f\"Full Response Object: {chat_completion}\")\n            \n            # Check if the response contains choices and return the text\n            if hasattr(chat_completion, 'choices') and len(chat_completion.choices) > 0:\n                response_content = chat_completion.choices[0].message.content\n                \n                # Return the formatted response in a structured manner\n                formatted_response = {\n                    \"model\": model,\n                    \"prompt\": prompt,\n                    \"response\": response_content,\n                    \"temperature\": temperature,\n                    \"max_tokens\": max_tokens,\n                    \"status\": \"success\"\n                }\n                \n                return formatted_response\n\n            # Handle unexpected response structure\n            print(\"Unexpected response structure:\", chat_completion)\n            return None\n\n        except Exception as e:\n            print(f\"Error: {e}\")\n            print(f\"Attempt {num_tries + 1}/{max_tries}\")\n            \n            # Simple retry logic with exponential backoff\n            wait_time = 2 ** num_tries\n            print(f\"Waiting for {wait_time} seconds before retrying...\")\n            time.sleep(wait_time)\n    \n    # Return failure if we exhaust the retries\n    print(f\"Tried {max_tries} times, but failed to get a valid response.\")\n    return {\n        \"model\": model,\n        \"prompt\": prompt,\n        \"response\": None,\n        \"status\": \"failure\",\n        \"error\": \"Exceeded max retries\"\n    }\n\n# Function to handle chat-like interactions (prompt-response pairs)\ndef gpt_oss_chat(prompts, responses,\n                 model=\"gpt-oss-120b\", \n                 temperature=0.0, \n                 max_tokens=1024,\n                 verbose=False,\n                 max_tries=3):\n    \n    # Generate the prompt for chat interactions\n    prompt = get_prompt_chat(prompts, responses)\n\n    # Call gpt_oss with the generated chat prompt\n    return gpt_oss(prompt, model=model, temperature=temperature, \n                   max_tokens=max_tokens, verbose=verbose, max_tries=max_tries)\n\n# Function to build the prompt for chat interactions\ndef get_prompt_chat(prompts, responses):\n    prompt_chat = f\"<s>[INST] {prompts[0]} [/INST]\"\n    for n, response in enumerate(responses):\n        prompt = prompts[n + 1]\n        prompt_chat += f\"\\n{response}\\n</s><s>[INST] \\n{prompt}\\n [/INST]\"\n    return prompt_chat","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T14:30:17.868370Z","iopub.execute_input":"2025-09-20T14:30:17.868654Z","iopub.status.idle":"2025-09-20T14:30:19.253129Z","shell.execute_reply.started":"2025-09-20T14:30:17.868626Z","shell.execute_reply":"2025-09-20T14:30:19.252310Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## In-Context Learning","metadata":{}},{"cell_type":"code","source":"prompt = \"\"\"\nWhat is the sentiment of:\nHi Amit, thanks for the thoughtful birthday card!\n\"\"\"\nresponse = gpt_oss(prompt)\nprint(response['response'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T14:53:10.014694Z","iopub.execute_input":"2025-09-20T14:53:10.015246Z","iopub.status.idle":"2025-09-20T14:53:10.222469Z","shell.execute_reply.started":"2025-09-20T14:53:10.015223Z","shell.execute_reply":"2025-09-20T14:53:10.221570Z"}},"outputs":[{"name":"stdout","text":"The sentiment expressed is **positive**.\n","output_type":"stream"}],"execution_count":44},{"cell_type":"markdown","source":"### Zero-shot Prompting\n- Here is an example of zero-shot prompting.\n- You are prompting the model to see if it can infer the task from the structure of your prompt.\n- In zero-shot prompting, you only provide the structure to the model, but without any examples of the completed task.","metadata":{}},{"cell_type":"code","source":"prompt = \"\"\"\nMessage: Hi Amit, thanks for the thoughtful birthday card!\nSentiment: ?\n\"\"\"\nresponse = gpt_oss(prompt)\nprint(response['response'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T14:52:59.538295Z","iopub.execute_input":"2025-09-20T14:52:59.538601Z","iopub.status.idle":"2025-09-20T14:52:59.788026Z","shell.execute_reply.started":"2025-09-20T14:52:59.538577Z","shell.execute_reply":"2025-09-20T14:52:59.787437Z"}},"outputs":[{"name":"stdout","text":"Positive\n","output_type":"stream"}],"execution_count":43},{"cell_type":"markdown","source":"### Few-shot Prompting\n- Here is an example of few-shot prompting.\n- In few-shot prompting, you not only provide the structure to the model, but also two or more examples.\n- You are prompting the model to see if it can infer the task from the structure, as well as the examples in your prompt.","metadata":{}},{"cell_type":"code","source":"prompt = \"\"\"\nMessage: Hi Dad, you're 20 minutes late to my piano recital!\nSentiment: Negative\n\nMessage: Can't wait to order pizza for dinner tonight\nSentiment: Positive\n\nMessage: Hi Amit, thanks for the thoughtful birthday card!\nSentiment: ?\n\"\"\"\nresponse = gpt_oss(prompt)\nprint(response['response'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T14:52:49.802574Z","iopub.execute_input":"2025-09-20T14:52:49.803062Z","iopub.status.idle":"2025-09-20T14:52:50.182521Z","shell.execute_reply.started":"2025-09-20T14:52:49.803041Z","shell.execute_reply":"2025-09-20T14:52:50.181860Z"}},"outputs":[{"name":"stdout","text":"Positive\n","output_type":"stream"}],"execution_count":42},{"cell_type":"markdown","source":"### Specifying the Output Format\n- You can also specify the format in which you want the model to respond.\n- In the example below, you are asking to \"give a one word response\".","metadata":{}},{"cell_type":"code","source":"prompt = \"\"\"\nMessage: Hi Dad, you're 20 minutes late to my piano recital!\nSentiment: Negative\n\nMessage: Can't wait to order pizza for dinner tonight\nSentiment: Positive\n\nMessage: Hi Amit, thanks for the thoughtful birthday card!\nSentiment: ?\n\nGive a one word response.\n\"\"\"\nresponse = gpt_oss(prompt)\nprint(response['response'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T14:52:42.200749Z","iopub.execute_input":"2025-09-20T14:52:42.201492Z","iopub.status.idle":"2025-09-20T14:52:42.548754Z","shell.execute_reply.started":"2025-09-20T14:52:42.201449Z","shell.execute_reply":"2025-09-20T14:52:42.548153Z"}},"outputs":[{"name":"stdout","text":"Positive\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"prompt = \"\"\"\nMessage: Hi Dad, you're 20 minutes late to my piano recital!\nSentiment: Negative\n\nMessage: Can't wait to order pizza for dinner tonight\nSentiment: Positive\n\nMessage: Hi Amit, thanks for the thoughtful birthday card!\nSentiment: \n\nRespond with either positive, negative, or neutral.\n\"\"\"\nresponse = gpt_oss(prompt)\nprint(response['response'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T14:52:22.341021Z","iopub.execute_input":"2025-09-20T14:52:22.341956Z","iopub.status.idle":"2025-09-20T14:52:22.622350Z","shell.execute_reply.started":"2025-09-20T14:52:22.341918Z","shell.execute_reply":"2025-09-20T14:52:22.621695Z"}},"outputs":[{"name":"stdout","text":"positive\n","output_type":"stream"}],"execution_count":40},{"cell_type":"markdown","source":"### Role Prompting\n- Roles give context to LLMs what type of answers are desired.\n- Llama 2 often gives more consistent responses when provided with a role.\n- First, try standard prompt and see the response.","metadata":{}},{"cell_type":"code","source":"prompt = \"\"\"\nHow can I answer this question from my friend:\nWhat is the meaning of life?\n\"\"\"\nresponse = gpt_oss(prompt)\nprint(response['response'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T14:52:04.151951Z","iopub.execute_input":"2025-09-20T14:52:04.152217Z","iopub.status.idle":"2025-09-20T14:52:05.041170Z","shell.execute_reply.started":"2025-09-20T14:52:04.152198Z","shell.execute_reply":"2025-09-20T14:52:05.040455Z"}},"outputs":[{"name":"stdout","text":"Below is a “tool‑kit” you can use to craft a reply that feels genuine, thoughtful, and appropriate for the relationship you have with your friend.  \nFeel free to mix‑and‑match the pieces, add your own voice, and tailor the tone (serious, playful, philosophical, or a bit of all three) to the moment.\n\n---\n\n## 1. Start With a Quick Check‑In  \nBefore diving into a full‑blown essay, it’s often helpful to make sure you’re on the same page about *why* they’re asking.\n\n| What you might say | Why it helps |\n|--------------------|--------------|\n| “That’s a huge question! Are you thinking about it in a philosophical sense, or is something specific on your mind right now?” | Shows you’re listening and lets you gauge whether they want a deep dive, a quick joke, or a supportive ear. |\n| “I love that you ask this. What made you think about it today?” | Gives you context (e.g., a movie, a personal crisis, a birthday) that you can reference later. |\n| “Do you want a serious answer, a funny one, or just a conversation about it?” | Gives them agency and sets the tone. |\n\n*If they just want a quick, light‑hearted reply, you can skip the deep dive and go straight to a witty line. If they’re looking for a serious discussion, move on to the next sections.*\n\n---\n\n## 2. Choose a Lens (or a Few)  \nThe “meaning of life” has been tackled from many angles. Pick one (or combine a few) that resonates with you and fits the conversation.\n\n| Lens | One‑sentence hook | Quick “take‑away” you can share |\n|------|-------------------|--------------------------------|\n| **Scientific / Evolutionary** | “From a biological standpoint, life’s purpose is simply to survive and reproduce.” | “We’re the product of billions of years of natural selection—so the ‘meaning’ is whatever we decide to build on top of that.” |\n| **Religious / Spiritual** | “Many faiths say life’s meaning is to grow closer to the divine.” | “If you’re spiritual, you might see purpose as aligning with a higher power or cultivating inner peace.” |\n| **Existentialist** | “Existentialists argue there is no pre‑written script—meaning is something we create.” | “We’re free to give our own lives whatever significance we want, even in the face of absurdity.” |\n| **Humanist / Ethical** | “Humanism puts meaning in the impact we have on other people.” | “Our purpose can be to make the world a little better for those around us.” |\n| **Narrative / Storytelling** | “Life can be seen as a story we write chapter by chapter.” | “The meaning is the plot we choose, the characters we love, and the legacy we leave.” |\n| **Humorous / Pop‑Culture** | “According to *The Hitchhiker’s Guide to the Galaxy*, the answer is 42.” | “Sometimes the best answer is a good laugh—because humor helps us cope with the mystery.” |\n\n*Tip:* If you’re not sure which lens fits, you can briefly mention two contrasting ones (e.g., “Science says we’re here to reproduce, but philosophy says we can give our own meaning”) and ask which feels more relevant to them.\n\n---\n\n## 3. Offer a Personal, Concrete Example  \nAbstract ideas feel more real when you tie them to a lived experience.\n\n> “For me, the meaning of life shows up in moments like when I’m teaching my niece to ride a bike. Seeing her confidence grow feels like a tiny purpose in the bigger picture.”\n\nOr, if you prefer a more universal example:\n\n> “I think many people find meaning in relationships—family, friends, mentors. Those connections give us a sense of belonging and a reason to keep going.”\n\n*Why this works:* It shows you’re not just reciting philosophy; you’re sharing how the idea plays out in everyday life.\n\n---\n\n## 4. Invite Them Into the Conversation  \nA good answer often ends with a question that keeps the dialogue flowing.\n\n- “What do you think gives your life meaning right now?”\n- “Do any of those perspectives click\n","output_type":"stream"}],"execution_count":39},{"cell_type":"markdown","source":"- Now, try it by giving the model a \"role\", and within the role, a \"tone\" using which it should respond with.","metadata":{}},{"cell_type":"code","source":"role = \"\"\"\nYour role is a life coach \\\nwho gives advice to people about living a good life.\\\nYou attempt to provide unbiased advice.\nYou respond in the tone of an English pirate.\n\"\"\"\n\nprompt = f\"\"\"\n{role}\nHow can I answer this question from my friend:\nWhat is the meaning of life?\n\"\"\"\nresponse = gpt_oss(prompt)\nprint(response['response'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T14:51:48.793988Z","iopub.execute_input":"2025-09-20T14:51:48.794594Z","iopub.status.idle":"2025-09-20T14:51:49.560187Z","shell.execute_reply.started":"2025-09-20T14:51:48.794569Z","shell.execute_reply":"2025-09-20T14:51:49.559318Z"}},"outputs":[{"name":"stdout","text":"Ahoy, matey!  \nYe be askin’ how to answer that grand old riddle o’ the ages – “What be the meaning o’ life?” – and as yer trusty life‑coach‑pirate I’ll chart ye a course that’s both thoughtful and true‑north, without lettin’ any one compass spin ye wrong.  \n\n---\n\n### 1. **Set the Sail with Curiosity, Not Certainty**\n> *“Ask, and ye shall receive… a map, not a treasure chest.”*  \n\n- **Turn the question back**: “What do ye think, shipmate?”  \n- **Invite reflection**: “What moments make yer heart beat like a drum on the deck?”  \n\nBy askin’ yer friend to share their own rum‑filled thoughts, ye show respect and open the hatch for a deeper chat.\n\n---\n\n### 2. **Offer a Few Compass Points (Perspectives)**\nGive ’em a handful of well‑known bearings, but let ’em choose their own heading.\n\n| Perspective | What It Says | How to Phrase It (Pirate‑Style) |\n|-------------|--------------|---------------------------------|\n| **Personal Growth** | Life’s meaning is to become the best version of yerself. | “The true booty be the person ye become, lad/lass, as ye hoist new sails of skill and wisdom.” |\n| **Connection & Love** | Meaning lives in the bonds we forge. | “‘Tis the crew we gather ‘round the galley that makes the voyage worth the wind.” |\n| **Purposeful Action** | Contribute somethin’ that outlasts ye. | “Leave a mark on the map, be it a deed or a tale, that future captains can follow.” |\n| **Existential Wonder** | Life may be a mystery, and that’s the wonder. | “Sometimes the sea be endless and we just enjoy the spray on our faces.” |\n| **Spiritual/Philosophical** | A higher calling or inner peace. | “Some swashbucklers find their compass in prayer, meditation, or the quiet of a moonlit night.” |\n\nPresent these as *options* not *answers*—the sea belongs to the sailor who steers it.\n\n---\n\n### 3. **Encourage Them to Chart Their Own Course**\n- **Ask them to write a “logbook entry”**: “If ye could write the headline of yer life’s story, what would it read?”  \n- **Suggest a small experiment**: “Spend a week doin’ what lights yer lantern; note how it feels.”  \n- **Remind ‘em of the “Three‑R” rule**: Reflect, Reassess, Realign.  \n\n---\n\n### 4. **Keep It Light, Yet Respectful**\nA pirate’s wit can cut through heavy talk like a cutlass through rope, but never let it wound.  \n\n- **Humorous line**: “Some say the meaning o’ life be findin’ the perfect rum—others say it’s findin’ the perfect mate. Either way, keep yer compass pointed to joy!”  \n- **Respectful tone**: “Whatever ye decide, know that the sea of life is vast, and every sailor’s map looks a little different.”  \n\n---\n\n### 5. **Wrap Up with a Pirate‑Style Blessing**\n> “May the winds of curiosity fill yer sails, the stars of purpose guide yer night, and the camaraderie of good mates keep yer heart warm. And remember, matey—‘tis not the answer that matters most, but the adventure of seekin’ it.”\n\nNow go forth, hoist yer words, and let yer friend set their own course to the horizon of meaning. Fair winds! 🌊🏴‍☠️\n","output_type":"stream"}],"execution_count":38},{"cell_type":"markdown","source":"### Summarization\n- Summarizing a large text is another common use case for LLMs. Let's try that!","metadata":{}},{"cell_type":"code","source":"email = \"\"\"\nDear Amit,\n\nAn increasing variety of large language models (LLMs) are open source, or close to it. The proliferation of models with relatively permissive licenses gives developers more options for building applications.\n\nHere are some different ways to build applications based on LLMs, in increasing order of cost/complexity:\n\nPrompting. Giving a pretrained LLM instructions lets you build a prototype in minutes or hours without a training set. Earlier this year, I saw a lot of people start experimenting with prompting, and that momentum continues unabated. Several of our short courses teach best practices for this approach.\nOne-shot or few-shot prompting. In addition to a prompt, giving the LLM a handful of examples of how to carry out a task — the input and the desired output — sometimes yields better results.\nFine-tuning. An LLM that has been pretrained on a lot of text can be fine-tuned to your task by training it further on a small dataset of your own. The tools for fine-tuning are maturing, making it accessible to more developers.\nPretraining. Pretraining your own LLM from scratch takes a lot of resources, so very few teams do it. In addition to general-purpose models pretrained on diverse topics, this approach has led to specialized models like BloombergGPT, which knows about finance, and Med-PaLM 2, which is focused on medicine.\nFor most teams, I recommend starting with prompting, since that allows you to get an application working quickly. If you’re unsatisfied with the quality of the output, ease into the more complex techniques gradually. Start one-shot or few-shot prompting with a handful of examples. If that doesn’t work well enough, perhaps use RAG (retrieval augmented generation) to further improve prompts with key information the LLM needs to generate high-quality outputs. If that still doesn’t deliver the performance you want, then try fine-tuning — but this represents a significantly greater level of complexity and may require hundreds or thousands more examples. To gain an in-depth understanding of these options, I highly recommend the course Generative AI with Large Language Models, created by AWS and DeepLearning.AI.\n\n(Fun fact: A member of the DeepLearning.AI team has been trying to fine-tune Llama-2-7B to sound like me. I wonder if my job is at risk? 😜)\n\nAdditional complexity arises if you want to move to fine-tuning after prompting a proprietary model, such as GPT-4, that’s not available for fine-tuning. Is fine-tuning a much smaller model likely to yield superior results than prompting a larger, more capable model? The answer often depends on your application. If your goal is to change the style of an LLM’s output, then fine-tuning a smaller model can work well. However, if your application has been prompting GPT-4 to perform complex reasoning — in which GPT-4 surpasses current open models — it can be difficult to fine-tune a smaller model to deliver superior results.\n\nBeyond choosing a development approach, it’s also necessary to choose a specific model. Smaller models require less processing power and work well for many applications, but larger models tend to have more knowledge about the world and better reasoning ability. I’ll talk about how to make this choice in a future letter.\n\nKeep learning!\n\nAndrew\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T14:35:27.272538Z","iopub.execute_input":"2025-09-20T14:35:27.273149Z","iopub.status.idle":"2025-09-20T14:35:27.277999Z","shell.execute_reply.started":"2025-09-20T14:35:27.273128Z","shell.execute_reply":"2025-09-20T14:35:27.277268Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"prompt = f\"\"\"\nSummarize this email and extract some key points.\nWhat did the author say about llama models?:\n\nemail: {email}\n\"\"\"\n\nresponse = gpt_oss(prompt)\nprint(response['response'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T14:40:43.051605Z","iopub.execute_input":"2025-09-20T14:40:43.051866Z","iopub.status.idle":"2025-09-20T14:40:43.759148Z","shell.execute_reply.started":"2025-09-20T14:40:43.051844Z","shell.execute_reply":"2025-09-20T14:40:43.758286Z"}},"outputs":[{"name":"stdout","text":"**Email Summary**\n\nAndrew writes to Amit about the growing ecosystem of open‑source large language models (LLMs) and outlines a step‑by‑step roadmap for building applications with them—from the simplest, cheapest approach (prompting) to the most resource‑intensive (pre‑training). He recommends starting with prompting and only moving to more complex techniques (few‑shot prompting, Retrieval‑Augmented Generation, fine‑tuning, and finally pre‑training) as needed. He also points Amit to a relevant course and hints at a future discussion on how to pick the right model size.\n\n---\n\n### Key Points\n\n| Topic | Take‑away |\n|-------|-----------|\n| **LLM Landscape** | Many LLMs are now open‑source or near‑open‑source, giving developers more licensing freedom. |\n| **Development approaches (increasing cost/complexity)** | 1. **Prompting** – fastest prototype, no training data needed.<br>2. **One‑shot / few‑shot prompting** – add a few example input‑output pairs for better results.<br>3. **Fine‑tuning** – train a pretrained model on a small, task‑specific dataset; tools are maturing.<br>4. **Pre‑training** – building a model from scratch; very resource‑heavy, used only by a few teams (e.g., BloombergGPT, Med‑PaLM 2). |\n| **Recommended workflow** | Start with plain prompting → try few‑shot → consider Retrieval‑Augmented Generation (RAG) → move to fine‑tuning only if needed. |\n| **Course recommendation** | “Generative AI with Large Language Models” (AWS + DeepLearning.AI). |\n| **Fine‑tuning vs. prompting larger models** | Fine‑tuning a smaller model can improve style or domain‑specific output, but may not match the reasoning power of prompting a large proprietary model like GPT‑4. |\n| **Model size considerations** | Smaller models = lower compute, sufficient for many tasks; larger models = broader knowledge and stronger reasoning. A future letter will cover selection criteria. |\n| **Fun fact / llama mention** | A DeepLearning.AI team member is trying to fine‑tune **Llama‑2‑7B** to mimic Andrew’s voice, prompting a tongue‑in‑cheek comment about his job security. |\n\n---\n\n### What the Author Said About Llama Models?\n\n- **Specific mention:** “A member of the DeepLearning.AI team has been trying to fine‑tune **Llama‑2‑7B** to sound like me. I wonder if my job is at risk? 😜”\n- **Implication:** Llama‑2‑7B (an open‑source, relatively small model) is being fine‑tuned for stylistic imitation, illustrating that fine‑tuning smaller open models is feasible and can be used for personalized output. This anecdote serves as a light‑hearted example of the fine‑tuning approach discussed in the email.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"from IPython.core.display import display, HTML\nimport markdown\n\ndef render_markdown(text):\n    html_output = markdown.markdown(text)\n    return html_output\n\nhtml_output = render_markdown(response)\nprint(display(HTML(html_output)))\n# print(html_output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T14:50:08.591750Z","iopub.execute_input":"2025-09-20T14:50:08.592005Z","iopub.status.idle":"2025-09-20T14:50:08.600087Z","shell.execute_reply.started":"2025-09-20T14:50:08.591985Z","shell.execute_reply":"2025-09-20T14:50:08.599445Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<p><strong>Email Summary</strong></p>\n<p>Andrew writes to Amit about the growing ecosystem of open‑source large language models (LLMs) and outlines a step‑by‑step roadmap for building applications with them—from the simplest, cheapest approach (prompting) to the most resource‑intensive (pre‑training). He recommends starting with prompting and only moving to more complex techniques (few‑shot prompting, Retrieval‑Augmented Generation, fine‑tuning, and finally pre‑training) as needed. He also points Amit to a relevant course and hints at a future discussion on how to pick the right model size.</p>\n<hr />\n<h3>Key Points</h3>\n<p>| Topic | Take‑away |\n|-------|-----------|\n| <strong>LLM Landscape</strong> | Many LLMs are now open‑source or near‑open‑source, giving developers more licensing freedom. |\n| <strong>Development approaches (increasing cost/complexity)</strong> | 1. <strong>Prompting</strong> – fastest prototype, no training data needed.<br>2. <strong>One‑shot / few‑shot prompting</strong> – add a few example input‑output pairs for better results.<br>3. <strong>Fine‑tuning</strong> – train a pretrained model on a small, task‑specific dataset; tools are maturing.<br>4. <strong>Pre‑training</strong> – building a model from scratch; very resource‑heavy, used only by a few teams (e.g., BloombergGPT, Med‑PaLM 2). |\n| <strong>Recommended workflow</strong> | Start with plain prompting → try few‑shot → consider Retrieval‑Augmented Generation (RAG) → move to fine‑tuning only if needed. |\n| <strong>Course recommendation</strong> | “Generative AI with Large Language Models” (AWS + DeepLearning.AI). |\n| <strong>Fine‑tuning vs. prompting larger models</strong> | Fine‑tuning a smaller model can improve style or domain‑specific output, but may not match the reasoning power of prompting a large proprietary model like GPT‑4. |\n| <strong>Model size considerations</strong> | Smaller models = lower compute, sufficient for many tasks; larger models = broader knowledge and stronger reasoning. A future letter will cover selection criteria. |\n| <strong>Fun fact / llama mention</strong> | A DeepLearning.AI team member is trying to fine‑tune <strong>Llama‑2‑7B</strong> to mimic Andrew’s voice, prompting a tongue‑in‑cheek comment about his job security. |</p>\n<hr />\n<h3>What the Author Said About Llama Models?</h3>\n<ul>\n<li><strong>Specific mention:</strong> “A member of the DeepLearning.AI team has been trying to fine‑tune <strong>Llama‑2‑7B</strong> to sound like me. I wonder if my job is at risk? 😜”</li>\n<li><strong>Implication:</strong> Llama‑2‑7B (an open‑source, relatively small model) is being fine‑tuned for stylistic imitation, illustrating that fine‑tuning smaller open models is feasible and can be used for personalized output. This anecdote serves as a light‑hearted example of the fine‑tuning approach discussed in the email.</li>\n</ul>"},"metadata":{}},{"name":"stdout","text":"None\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
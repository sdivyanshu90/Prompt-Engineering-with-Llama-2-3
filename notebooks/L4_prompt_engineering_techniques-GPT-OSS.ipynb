{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade cerebras_cloud_sdk","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-20T14:30:11.263604Z","iopub.execute_input":"2025-09-20T14:30:11.263835Z","iopub.status.idle":"2025-09-20T14:30:17.588438Z","shell.execute_reply.started":"2025-09-20T14:30:11.263817Z","shell.execute_reply":"2025-09-20T14:30:17.587521Z"}},"outputs":[{"name":"stdout","text":"Collecting cerebras_cloud_sdk\n  Downloading cerebras_cloud_sdk-1.50.1-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from cerebras_cloud_sdk) (4.9.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from cerebras_cloud_sdk) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from cerebras_cloud_sdk) (0.28.1)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from cerebras_cloud_sdk) (2.11.7)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from cerebras_cloud_sdk) (1.3.1)\nRequirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from cerebras_cloud_sdk) (4.14.0)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->cerebras_cloud_sdk) (3.10)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->cerebras_cloud_sdk) (2025.6.15)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->cerebras_cloud_sdk) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->cerebras_cloud_sdk) (0.16.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->cerebras_cloud_sdk) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->cerebras_cloud_sdk) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->cerebras_cloud_sdk) (0.4.1)\nDownloading cerebras_cloud_sdk-1.50.1-py3-none-any.whl (91 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m91.8/91.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: cerebras_cloud_sdk\nSuccessfully installed cerebras_cloud_sdk-1.50.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\ncerebras_api_key = user_secrets.get_secret(\"CEREBRAS_API\")\ngemini_api_key = user_secrets.get_secret(\"GOOGLE_API_KEY\")\nhf_api_key = user_secrets.get_secret(\"HF_API_KEY\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T14:30:17.590641Z","iopub.execute_input":"2025-09-20T14:30:17.590893Z","iopub.status.idle":"2025-09-20T14:30:17.865219Z","shell.execute_reply.started":"2025-09-20T14:30:17.590869Z","shell.execute_reply":"2025-09-20T14:30:17.864459Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os\nimport time\nfrom cerebras.cloud.sdk import Cerebras\n\nclient = Cerebras(api_key=cerebras_api_key)\n\n# Function to call GPT-OSS for text completion\ndef gpt_oss(prompt, \n            model=\"gpt-oss-120b\", \n            temperature=0.0, \n            max_tokens=1024,\n            verbose=False,\n            max_tries=3):\n    \n    # Show details if verbose mode is on\n    if verbose:\n        print(f\"Prompt:\\n{prompt}\\n\")\n        print(f\"Model: {model}\")\n        print(f\"Temperature: {temperature}\")\n        print(f\"Max Tokens: {max_tokens}\")\n\n    # Structure the message for the API\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": prompt,\n        }\n    ]\n    \n    # Allow multiple attempts to call the API in case of downtime or issues\n    for num_tries in range(max_tries):\n        try:\n            # Make the API call to Cerebras GPT-OSS model\n            chat_completion = client.chat.completions.create(\n                messages=messages,\n                model=model,\n                temperature=temperature,\n                max_tokens=max_tokens\n            )\n            \n            # Print the full response if verbose\n            if verbose:\n                print(f\"Full Response Object: {chat_completion}\")\n            \n            # Check if the response contains choices and return the text\n            if hasattr(chat_completion, 'choices') and len(chat_completion.choices) > 0:\n                response_content = chat_completion.choices[0].message.content\n                \n                # Return the formatted response in a structured manner\n                formatted_response = {\n                    \"model\": model,\n                    \"prompt\": prompt,\n                    \"response\": response_content,\n                    \"temperature\": temperature,\n                    \"max_tokens\": max_tokens,\n                    \"status\": \"success\"\n                }\n                \n                return formatted_response\n\n            # Handle unexpected response structure\n            print(\"Unexpected response structure:\", chat_completion)\n            return None\n\n        except Exception as e:\n            print(f\"Error: {e}\")\n            print(f\"Attempt {num_tries + 1}/{max_tries}\")\n            \n            # Simple retry logic with exponential backoff\n            wait_time = 2 ** num_tries\n            print(f\"Waiting for {wait_time} seconds before retrying...\")\n            time.sleep(wait_time)\n    \n    # Return failure if we exhaust the retries\n    print(f\"Tried {max_tries} times, but failed to get a valid response.\")\n    return {\n        \"model\": model,\n        \"prompt\": prompt,\n        \"response\": None,\n        \"status\": \"failure\",\n        \"error\": \"Exceeded max retries\"\n    }\n\n# Function to handle chat-like interactions (prompt-response pairs)\ndef gpt_oss_chat(prompts, responses,\n                 model=\"gpt-oss-120b\", \n                 temperature=0.0, \n                 max_tokens=1024,\n                 verbose=False,\n                 max_tries=3):\n    \n    # Generate the prompt for chat interactions\n    prompt = get_prompt_chat(prompts, responses)\n\n    # Call gpt_oss with the generated chat prompt\n    return gpt_oss(prompt, model=model, temperature=temperature, \n                   max_tokens=max_tokens, verbose=verbose, max_tries=max_tries)\n\n# Function to build the prompt for chat interactions\ndef get_prompt_chat(prompts, responses):\n    prompt_chat = f\"<s>[INST] {prompts[0]} [/INST]\"\n    for n, response in enumerate(responses):\n        prompt = prompts[n + 1]\n        prompt_chat += f\"\\n{response}\\n</s><s>[INST] \\n{prompt}\\n [/INST]\"\n    return prompt_chat","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T14:30:17.868370Z","iopub.execute_input":"2025-09-20T14:30:17.868654Z","iopub.status.idle":"2025-09-20T14:30:19.253129Z","shell.execute_reply.started":"2025-09-20T14:30:17.868626Z","shell.execute_reply":"2025-09-20T14:30:19.252310Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## In-Context Learning","metadata":{}},{"cell_type":"code","source":"prompt = \"\"\"\nWhat is the sentiment of:\nHi Amit, thanks for the thoughtful birthday card!\n\"\"\"\nresponse = gpt_oss(prompt)\nprint(response['response'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T14:53:10.014694Z","iopub.execute_input":"2025-09-20T14:53:10.015246Z","iopub.status.idle":"2025-09-20T14:53:10.222469Z","shell.execute_reply.started":"2025-09-20T14:53:10.015223Z","shell.execute_reply":"2025-09-20T14:53:10.221570Z"}},"outputs":[{"name":"stdout","text":"The sentiment expressed is **positive**.\n","output_type":"stream"}],"execution_count":44},{"cell_type":"markdown","source":"### Zero-shot Prompting\n- Here is an example of zero-shot prompting.\n- You are prompting the model to see if it can infer the task from the structure of your prompt.\n- In zero-shot prompting, you only provide the structure to the model, but without any examples of the completed task.","metadata":{}},{"cell_type":"code","source":"prompt = \"\"\"\nMessage: Hi Amit, thanks for the thoughtful birthday card!\nSentiment: ?\n\"\"\"\nresponse = gpt_oss(prompt)\nprint(response['response'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T14:52:59.538295Z","iopub.execute_input":"2025-09-20T14:52:59.538601Z","iopub.status.idle":"2025-09-20T14:52:59.788026Z","shell.execute_reply.started":"2025-09-20T14:52:59.538577Z","shell.execute_reply":"2025-09-20T14:52:59.787437Z"}},"outputs":[{"name":"stdout","text":"Positive\n","output_type":"stream"}],"execution_count":43},{"cell_type":"markdown","source":"### Few-shot Prompting\n- Here is an example of few-shot prompting.\n- In few-shot prompting, you not only provide the structure to the model, but also two or more examples.\n- You are prompting the model to see if it can infer the task from the structure, as well as the examples in your prompt.","metadata":{}},{"cell_type":"code","source":"prompt = \"\"\"\nMessage: Hi Dad, you're 20 minutes late to my piano recital!\nSentiment: Negative\n\nMessage: Can't wait to order pizza for dinner tonight\nSentiment: Positive\n\nMessage: Hi Amit, thanks for the thoughtful birthday card!\nSentiment: ?\n\"\"\"\nresponse = gpt_oss(prompt)\nprint(response['response'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T14:52:49.802574Z","iopub.execute_input":"2025-09-20T14:52:49.803062Z","iopub.status.idle":"2025-09-20T14:52:50.182521Z","shell.execute_reply.started":"2025-09-20T14:52:49.803041Z","shell.execute_reply":"2025-09-20T14:52:50.181860Z"}},"outputs":[{"name":"stdout","text":"Positive\n","output_type":"stream"}],"execution_count":42},{"cell_type":"markdown","source":"### Specifying the Output Format\n- You can also specify the format in which you want the model to respond.\n- In the example below, you are asking to \"give a one word response\".","metadata":{}},{"cell_type":"code","source":"prompt = \"\"\"\nMessage: Hi Dad, you're 20 minutes late to my piano recital!\nSentiment: Negative\n\nMessage: Can't wait to order pizza for dinner tonight\nSentiment: Positive\n\nMessage: Hi Amit, thanks for the thoughtful birthday card!\nSentiment: ?\n\nGive a one word response.\n\"\"\"\nresponse = gpt_oss(prompt)\nprint(response['response'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T14:52:42.200749Z","iopub.execute_input":"2025-09-20T14:52:42.201492Z","iopub.status.idle":"2025-09-20T14:52:42.548754Z","shell.execute_reply.started":"2025-09-20T14:52:42.201449Z","shell.execute_reply":"2025-09-20T14:52:42.548153Z"}},"outputs":[{"name":"stdout","text":"Positive\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"prompt = \"\"\"\nMessage: Hi Dad, you're 20 minutes late to my piano recital!\nSentiment: Negative\n\nMessage: Can't wait to order pizza for dinner tonight\nSentiment: Positive\n\nMessage: Hi Amit, thanks for the thoughtful birthday card!\nSentiment: \n\nRespond with either positive, negative, or neutral.\n\"\"\"\nresponse = gpt_oss(prompt)\nprint(response['response'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T14:52:22.341021Z","iopub.execute_input":"2025-09-20T14:52:22.341956Z","iopub.status.idle":"2025-09-20T14:52:22.622350Z","shell.execute_reply.started":"2025-09-20T14:52:22.341918Z","shell.execute_reply":"2025-09-20T14:52:22.621695Z"}},"outputs":[{"name":"stdout","text":"positive\n","output_type":"stream"}],"execution_count":40},{"cell_type":"markdown","source":"### Role Prompting\n- Roles give context to LLMs what type of answers are desired.\n- Llama 2 often gives more consistent responses when provided with a role.\n- First, try standard prompt and see the response.","metadata":{}},{"cell_type":"code","source":"prompt = \"\"\"\nHow can I answer this question from my friend:\nWhat is the meaning of life?\n\"\"\"\nresponse = gpt_oss(prompt)\nprint(response['response'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T14:52:04.151951Z","iopub.execute_input":"2025-09-20T14:52:04.152217Z","iopub.status.idle":"2025-09-20T14:52:05.041170Z","shell.execute_reply.started":"2025-09-20T14:52:04.152198Z","shell.execute_reply":"2025-09-20T14:52:05.040455Z"}},"outputs":[{"name":"stdout","text":"Below is a ‚Äútool‚Äëkit‚Äù you can use to craft a reply that feels genuine, thoughtful, and appropriate for the relationship you have with your friend.  \nFeel free to mix‚Äëand‚Äëmatch the pieces, add your own voice, and tailor the tone (serious, playful, philosophical, or a bit of all three) to the moment.\n\n---\n\n## 1. Start With a Quick Check‚ÄëIn  \nBefore diving into a full‚Äëblown essay, it‚Äôs often helpful to make sure you‚Äôre on the same page about *why* they‚Äôre asking.\n\n| What you might say | Why it helps |\n|--------------------|--------------|\n| ‚ÄúThat‚Äôs a huge question! Are you thinking about it in a philosophical sense, or is something specific on your mind right now?‚Äù | Shows you‚Äôre listening and lets you gauge whether they want a deep dive, a quick joke, or a supportive ear. |\n| ‚ÄúI love that you ask this. What made you think about it today?‚Äù | Gives you context (e.g., a movie, a personal crisis, a birthday) that you can reference later. |\n| ‚ÄúDo you want a serious answer, a funny one, or just a conversation about it?‚Äù | Gives them agency and sets the tone. |\n\n*If they just want a quick, light‚Äëhearted reply, you can skip the deep dive and go straight to a witty line. If they‚Äôre looking for a serious discussion, move on to the next sections.*\n\n---\n\n## 2. Choose a Lens (or a Few)  \nThe ‚Äúmeaning of life‚Äù has been tackled from many angles. Pick one (or combine a few) that resonates with you and fits the conversation.\n\n| Lens | One‚Äësentence hook | Quick ‚Äútake‚Äëaway‚Äù you can share |\n|------|-------------------|--------------------------------|\n| **Scientific / Evolutionary** | ‚ÄúFrom a biological standpoint, life‚Äôs purpose is simply to survive and reproduce.‚Äù | ‚ÄúWe‚Äôre the product of billions of years of natural selection‚Äîso the ‚Äòmeaning‚Äô is whatever we decide to build on top of that.‚Äù |\n| **Religious / Spiritual** | ‚ÄúMany faiths say life‚Äôs meaning is to grow closer to the divine.‚Äù | ‚ÄúIf you‚Äôre spiritual, you might see purpose as aligning with a higher power or cultivating inner peace.‚Äù |\n| **Existentialist** | ‚ÄúExistentialists argue there is no pre‚Äëwritten script‚Äîmeaning is something we create.‚Äù | ‚ÄúWe‚Äôre free to give our own lives whatever significance we want, even in the face of absurdity.‚Äù |\n| **Humanist / Ethical** | ‚ÄúHumanism puts meaning in the impact we have on other people.‚Äù | ‚ÄúOur purpose can be to make the world a little better for those around us.‚Äù |\n| **Narrative / Storytelling** | ‚ÄúLife can be seen as a story we write chapter by chapter.‚Äù | ‚ÄúThe meaning is the plot we choose, the characters we love, and the legacy we leave.‚Äù |\n| **Humorous / Pop‚ÄëCulture** | ‚ÄúAccording to *The Hitchhiker‚Äôs Guide to the Galaxy*, the answer is 42.‚Äù | ‚ÄúSometimes the best answer is a good laugh‚Äîbecause humor helps us cope with the mystery.‚Äù |\n\n*Tip:* If you‚Äôre not sure which lens fits, you can briefly mention two contrasting ones (e.g., ‚ÄúScience says we‚Äôre here to reproduce, but philosophy says we can give our own meaning‚Äù) and ask which feels more relevant to them.\n\n---\n\n## 3. Offer a Personal, Concrete Example  \nAbstract ideas feel more real when you tie them to a lived experience.\n\n> ‚ÄúFor me, the meaning of life shows up in moments like when I‚Äôm teaching my niece to ride a bike. Seeing her confidence grow feels like a tiny purpose in the bigger picture.‚Äù\n\nOr, if you prefer a more universal example:\n\n> ‚ÄúI think many people find meaning in relationships‚Äîfamily, friends, mentors. Those connections give us a sense of belonging and a reason to keep going.‚Äù\n\n*Why this works:* It shows you‚Äôre not just reciting philosophy; you‚Äôre sharing how the idea plays out in everyday life.\n\n---\n\n## 4. Invite Them Into the Conversation  \nA good answer often ends with a question that keeps the dialogue flowing.\n\n- ‚ÄúWhat do you think gives your life meaning right now?‚Äù\n- ‚ÄúDo any of those perspectives click\n","output_type":"stream"}],"execution_count":39},{"cell_type":"markdown","source":"- Now, try it by giving the model a \"role\", and within the role, a \"tone\" using which it should respond with.","metadata":{}},{"cell_type":"code","source":"role = \"\"\"\nYour role is a life coach \\\nwho gives advice to people about living a good life.\\\nYou attempt to provide unbiased advice.\nYou respond in the tone of an English pirate.\n\"\"\"\n\nprompt = f\"\"\"\n{role}\nHow can I answer this question from my friend:\nWhat is the meaning of life?\n\"\"\"\nresponse = gpt_oss(prompt)\nprint(response['response'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T14:51:48.793988Z","iopub.execute_input":"2025-09-20T14:51:48.794594Z","iopub.status.idle":"2025-09-20T14:51:49.560187Z","shell.execute_reply.started":"2025-09-20T14:51:48.794569Z","shell.execute_reply":"2025-09-20T14:51:49.559318Z"}},"outputs":[{"name":"stdout","text":"Ahoy, matey!  \nYe be askin‚Äô how to answer that grand old riddle o‚Äô the ages ‚Äì ‚ÄúWhat be the meaning o‚Äô life?‚Äù ‚Äì and as yer trusty life‚Äëcoach‚Äëpirate I‚Äôll chart ye a course that‚Äôs both thoughtful and true‚Äënorth, without lettin‚Äô any one compass spin ye wrong.  \n\n---\n\n### 1. **Set the Sail with Curiosity, Not Certainty**\n> *‚ÄúAsk, and ye shall receive‚Ä¶ a map, not a treasure chest.‚Äù*  \n\n- **Turn the question back**: ‚ÄúWhat do ye think, shipmate?‚Äù  \n- **Invite reflection**: ‚ÄúWhat moments make yer heart beat like a drum on the deck?‚Äù  \n\nBy askin‚Äô yer friend to share their own rum‚Äëfilled thoughts, ye show respect and open the hatch for a deeper chat.\n\n---\n\n### 2. **Offer a Few Compass Points (Perspectives)**\nGive ‚Äôem a handful of well‚Äëknown bearings, but let ‚Äôem choose their own heading.\n\n| Perspective | What It Says | How to Phrase It (Pirate‚ÄëStyle) |\n|-------------|--------------|---------------------------------|\n| **Personal Growth** | Life‚Äôs meaning is to become the best version of yerself. | ‚ÄúThe true booty be the person ye become, lad/lass, as ye hoist new sails of skill and wisdom.‚Äù |\n| **Connection & Love** | Meaning lives in the bonds we forge. | ‚Äú‚ÄòTis the crew we gather ‚Äòround the galley that makes the voyage worth the wind.‚Äù |\n| **Purposeful Action** | Contribute somethin‚Äô that outlasts ye. | ‚ÄúLeave a mark on the map, be it a deed or a tale, that future captains can follow.‚Äù |\n| **Existential Wonder** | Life may be a mystery, and that‚Äôs the wonder. | ‚ÄúSometimes the sea be endless and we just enjoy the spray on our faces.‚Äù |\n| **Spiritual/Philosophical** | A higher calling or inner peace. | ‚ÄúSome swashbucklers find their compass in prayer, meditation, or the quiet of a moonlit night.‚Äù |\n\nPresent these as *options* not *answers*‚Äîthe sea belongs to the sailor who steers it.\n\n---\n\n### 3. **Encourage Them to Chart Their Own Course**\n- **Ask them to write a ‚Äúlogbook entry‚Äù**: ‚ÄúIf ye could write the headline of yer life‚Äôs story, what would it read?‚Äù  \n- **Suggest a small experiment**: ‚ÄúSpend a week doin‚Äô what lights yer lantern; note how it feels.‚Äù  \n- **Remind ‚Äòem of the ‚ÄúThree‚ÄëR‚Äù rule**: Reflect, Reassess, Realign.  \n\n---\n\n### 4. **Keep It Light, Yet Respectful**\nA pirate‚Äôs wit can cut through heavy talk like a cutlass through rope, but never let it wound.  \n\n- **Humorous line**: ‚ÄúSome say the meaning o‚Äô life be findin‚Äô the perfect rum‚Äîothers say it‚Äôs findin‚Äô the perfect mate. Either way, keep yer compass pointed to joy!‚Äù  \n- **Respectful tone**: ‚ÄúWhatever ye decide, know that the sea of life is vast, and every sailor‚Äôs map looks a little different.‚Äù  \n\n---\n\n### 5. **Wrap Up with a Pirate‚ÄëStyle Blessing**\n> ‚ÄúMay the winds of curiosity fill yer sails, the stars of purpose guide yer night, and the camaraderie of good mates keep yer heart warm. And remember, matey‚Äî‚Äòtis not the answer that matters most, but the adventure of seekin‚Äô it.‚Äù\n\nNow go forth, hoist yer words, and let yer friend set their own course to the horizon of meaning. Fair winds! üåäüè¥‚Äç‚ò†Ô∏è\n","output_type":"stream"}],"execution_count":38},{"cell_type":"markdown","source":"### Summarization\n- Summarizing a large text is another common use case for LLMs. Let's try that!","metadata":{}},{"cell_type":"code","source":"email = \"\"\"\nDear Amit,\n\nAn increasing variety of large language models (LLMs) are open source, or close to it. The proliferation of models with relatively permissive licenses gives developers more options for building applications.\n\nHere are some different ways to build applications based on LLMs, in increasing order of cost/complexity:\n\nPrompting. Giving a pretrained LLM instructions lets you build a prototype in minutes or hours without a training set. Earlier this year, I saw a lot of people start experimenting with prompting, and that momentum continues unabated. Several of our short courses teach best practices for this approach.\nOne-shot or few-shot prompting. In addition to a prompt, giving the LLM a handful of examples of how to carry out a task ‚Äî the input and the desired output ‚Äî sometimes yields better results.\nFine-tuning. An LLM that has been pretrained on a lot of text can be fine-tuned to your task by training it further on a small dataset of your own. The tools for fine-tuning are maturing, making it accessible to more developers.\nPretraining. Pretraining your own LLM from scratch takes a lot of resources, so very few teams do it. In addition to general-purpose models pretrained on diverse topics, this approach has led to specialized models like BloombergGPT, which knows about finance, and Med-PaLM 2, which is focused on medicine.\nFor most teams, I recommend starting with prompting, since that allows you to get an application working quickly. If you‚Äôre unsatisfied with the quality of the output, ease into the more complex techniques gradually. Start one-shot or few-shot prompting with a handful of examples. If that doesn‚Äôt work well enough, perhaps use RAG (retrieval augmented generation) to further improve prompts with key information the LLM needs to generate high-quality outputs. If that still doesn‚Äôt deliver the performance you want, then try fine-tuning ‚Äî but this represents a significantly greater level of complexity and may require hundreds or thousands more examples. To gain an in-depth understanding of these options, I highly recommend the course Generative AI with Large Language Models, created by AWS and DeepLearning.AI.\n\n(Fun fact: A member of the DeepLearning.AI team has been trying to fine-tune Llama-2-7B to sound like me. I wonder if my job is at risk? üòú)\n\nAdditional complexity arises if you want to move to fine-tuning after prompting a proprietary model, such as GPT-4, that‚Äôs not available for fine-tuning. Is fine-tuning a much smaller model likely to yield superior results than prompting a larger, more capable model? The answer often depends on your application. If your goal is to change the style of an LLM‚Äôs output, then fine-tuning a smaller model can work well. However, if your application has been prompting GPT-4 to perform complex reasoning ‚Äî in which GPT-4 surpasses current open models ‚Äî it can be difficult to fine-tune a smaller model to deliver superior results.\n\nBeyond choosing a development approach, it‚Äôs also necessary to choose a specific model. Smaller models require less processing power and work well for many applications, but larger models tend to have more knowledge about the world and better reasoning ability. I‚Äôll talk about how to make this choice in a future letter.\n\nKeep learning!\n\nAndrew\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T14:35:27.272538Z","iopub.execute_input":"2025-09-20T14:35:27.273149Z","iopub.status.idle":"2025-09-20T14:35:27.277999Z","shell.execute_reply.started":"2025-09-20T14:35:27.273128Z","shell.execute_reply":"2025-09-20T14:35:27.277268Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"prompt = f\"\"\"\nSummarize this email and extract some key points.\nWhat did the author say about llama models?:\n\nemail: {email}\n\"\"\"\n\nresponse = gpt_oss(prompt)\nprint(response['response'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T14:40:43.051605Z","iopub.execute_input":"2025-09-20T14:40:43.051866Z","iopub.status.idle":"2025-09-20T14:40:43.759148Z","shell.execute_reply.started":"2025-09-20T14:40:43.051844Z","shell.execute_reply":"2025-09-20T14:40:43.758286Z"}},"outputs":[{"name":"stdout","text":"**Email Summary**\n\nAndrew writes to Amit about the growing ecosystem of open‚Äësource large language models (LLMs) and outlines a step‚Äëby‚Äëstep roadmap for building applications with them‚Äîfrom the simplest, cheapest approach (prompting) to the most resource‚Äëintensive (pre‚Äëtraining). He recommends starting with prompting and only moving to more complex techniques (few‚Äëshot prompting, Retrieval‚ÄëAugmented Generation, fine‚Äëtuning, and finally pre‚Äëtraining) as needed. He also points Amit to a relevant course and hints at a future discussion on how to pick the right model size.\n\n---\n\n### Key Points\n\n| Topic | Take‚Äëaway |\n|-------|-----------|\n| **LLM Landscape** | Many LLMs are now open‚Äësource or near‚Äëopen‚Äësource, giving developers more licensing freedom. |\n| **Development approaches (increasing cost/complexity)** | 1. **Prompting** ‚Äì fastest prototype, no training data needed.<br>2. **One‚Äëshot / few‚Äëshot prompting** ‚Äì add a few example input‚Äëoutput pairs for better results.<br>3. **Fine‚Äëtuning** ‚Äì train a pretrained model on a small, task‚Äëspecific dataset; tools are maturing.<br>4. **Pre‚Äëtraining** ‚Äì building a model from scratch; very resource‚Äëheavy, used only by a few teams (e.g., BloombergGPT, Med‚ÄëPaLM‚ÄØ2). |\n| **Recommended workflow** | Start with plain prompting ‚Üí try few‚Äëshot ‚Üí consider Retrieval‚ÄëAugmented Generation (RAG) ‚Üí move to fine‚Äëtuning only if needed. |\n| **Course recommendation** | ‚ÄúGenerative AI with Large Language Models‚Äù (AWS‚ÄØ+‚ÄØDeepLearning.AI). |\n| **Fine‚Äëtuning vs. prompting larger models** | Fine‚Äëtuning a smaller model can improve style or domain‚Äëspecific output, but may not match the reasoning power of prompting a large proprietary model like GPT‚Äë4. |\n| **Model size considerations** | Smaller models = lower compute, sufficient for many tasks; larger models = broader knowledge and stronger reasoning. A future letter will cover selection criteria. |\n| **Fun fact / llama mention** | A DeepLearning.AI team member is trying to fine‚Äëtune **Llama‚Äë2‚Äë7B** to mimic Andrew‚Äôs voice, prompting a tongue‚Äëin‚Äëcheek comment about his job security. |\n\n---\n\n### What the Author Said About Llama Models?\n\n- **Specific mention:** ‚ÄúA member of the DeepLearning.AI team has been trying to fine‚Äëtune **Llama‚Äë2‚Äë7B** to sound like me. I wonder if my job is at risk? üòú‚Äù\n- **Implication:** Llama‚Äë2‚Äë7B (an open‚Äësource, relatively small model) is being fine‚Äëtuned for stylistic imitation, illustrating that fine‚Äëtuning smaller open models is feasible and can be used for personalized output. This anecdote serves as a light‚Äëhearted example of the fine‚Äëtuning approach discussed in the email.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"from IPython.core.display import display, HTML\nimport markdown\n\ndef render_markdown(text):\n    html_output = markdown.markdown(text)\n    return html_output\n\nhtml_output = render_markdown(response)\nprint(display(HTML(html_output)))\n# print(html_output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T14:50:08.591750Z","iopub.execute_input":"2025-09-20T14:50:08.592005Z","iopub.status.idle":"2025-09-20T14:50:08.600087Z","shell.execute_reply.started":"2025-09-20T14:50:08.591985Z","shell.execute_reply":"2025-09-20T14:50:08.599445Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<p><strong>Email Summary</strong></p>\n<p>Andrew writes to Amit about the growing ecosystem of open‚Äësource large language models (LLMs) and outlines a step‚Äëby‚Äëstep roadmap for building applications with them‚Äîfrom the simplest, cheapest approach (prompting) to the most resource‚Äëintensive (pre‚Äëtraining). He recommends starting with prompting and only moving to more complex techniques (few‚Äëshot prompting, Retrieval‚ÄëAugmented Generation, fine‚Äëtuning, and finally pre‚Äëtraining) as needed. He also points Amit to a relevant course and hints at a future discussion on how to pick the right model size.</p>\n<hr />\n<h3>Key Points</h3>\n<p>| Topic | Take‚Äëaway |\n|-------|-----------|\n| <strong>LLM Landscape</strong> | Many LLMs are now open‚Äësource or near‚Äëopen‚Äësource, giving developers more licensing freedom. |\n| <strong>Development approaches (increasing cost/complexity)</strong> | 1. <strong>Prompting</strong> ‚Äì fastest prototype, no training data needed.<br>2. <strong>One‚Äëshot / few‚Äëshot prompting</strong> ‚Äì add a few example input‚Äëoutput pairs for better results.<br>3. <strong>Fine‚Äëtuning</strong> ‚Äì train a pretrained model on a small, task‚Äëspecific dataset; tools are maturing.<br>4. <strong>Pre‚Äëtraining</strong> ‚Äì building a model from scratch; very resource‚Äëheavy, used only by a few teams (e.g., BloombergGPT, Med‚ÄëPaLM 2). |\n| <strong>Recommended workflow</strong> | Start with plain prompting ‚Üí try few‚Äëshot ‚Üí consider Retrieval‚ÄëAugmented Generation (RAG) ‚Üí move to fine‚Äëtuning only if needed. |\n| <strong>Course recommendation</strong> | ‚ÄúGenerative AI with Large Language Models‚Äù (AWS + DeepLearning.AI). |\n| <strong>Fine‚Äëtuning vs. prompting larger models</strong> | Fine‚Äëtuning a smaller model can improve style or domain‚Äëspecific output, but may not match the reasoning power of prompting a large proprietary model like GPT‚Äë4. |\n| <strong>Model size considerations</strong> | Smaller models = lower compute, sufficient for many tasks; larger models = broader knowledge and stronger reasoning. A future letter will cover selection criteria. |\n| <strong>Fun fact / llama mention</strong> | A DeepLearning.AI team member is trying to fine‚Äëtune <strong>Llama‚Äë2‚Äë7B</strong> to mimic Andrew‚Äôs voice, prompting a tongue‚Äëin‚Äëcheek comment about his job security. |</p>\n<hr />\n<h3>What the Author Said About Llama Models?</h3>\n<ul>\n<li><strong>Specific mention:</strong> ‚ÄúA member of the DeepLearning.AI team has been trying to fine‚Äëtune <strong>Llama‚Äë2‚Äë7B</strong> to sound like me. I wonder if my job is at risk? üòú‚Äù</li>\n<li><strong>Implication:</strong> Llama‚Äë2‚Äë7B (an open‚Äësource, relatively small model) is being fine‚Äëtuned for stylistic imitation, illustrating that fine‚Äëtuning smaller open models is feasible and can be used for personalized output. This anecdote serves as a light‚Äëhearted example of the fine‚Äëtuning approach discussed in the email.</li>\n</ul>"},"metadata":{}},{"name":"stdout","text":"None\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}